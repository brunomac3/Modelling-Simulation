{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5942c2-ffe7-4f30-85e4-df43472c80bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Baseline Evaluation ---\n",
      "Running 10 episodes with a RANDOM AGENT.\n",
      "The simulation window will open. It will be fast because the agent crashes quickly!\n",
      "  ... Episode 10 / 10 complete.\n",
      "\n",
      "--- Evaluation Complete. ---\n",
      "Raw results for all 10 episodes saved to: random_baseline_agent/instant_runs/run_20251114_123336.csv\n",
      "\n",
      "--- Final Metrics Summary (Random Agent Baseline) ---\n",
      "Summary metrics file saved to: random_baseline_agent/summary/summary_20251114_123336.csv\n",
      "Collision Rate               |   0.9000\n",
      "Average Speed (m/s)          |   0.3110\n",
      "Avg Lane Changes / Episode   |   6.4000\n",
      "Average Reward / Episode     |  12.9860\n",
      "Speed Limit Compliance       |   1.0000\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "N_EPISODES = 50  # How many full episodes to run\n",
    "SPEED_LIMIT_MS = 30.0 # The default speed limit in highway-v0 is 30 m/s\n",
    "BASE_OUTPUT_DIR = \"random_baseline_agent\" # Base directory for all results\n",
    "SAFE_TTC_THRESHOLD = 2.0  # Seconds - below this is considered unsafe\n",
    "\n",
    "# --- Setup ---\n",
    "RAW_RUNS_DIR = os.path.join(BASE_OUTPUT_DIR, \"instant_runs\")\n",
    "SUMMARY_DIR = os.path.join(BASE_OUTPUT_DIR, \"summary\")\n",
    "\n",
    "# Create all directories if they don't exist\n",
    "os.makedirs(RAW_RUNS_DIR, exist_ok=True)\n",
    "os.makedirs(SUMMARY_DIR, exist_ok=True)\n",
    "\n",
    "# --- Data Storage ---\n",
    "all_episode_stats = []\n",
    "\n",
    "print(f\"--- Starting ENHANCED Baseline Evaluation ---\")\n",
    "print(f\"Running {N_EPISODES} episodes with a RANDOM AGENT.\")\n",
    "print(f\"Now capturing: TTC, Jerk, Episode Duration, and more!\")\n",
    "print(\"The simulation window will open. It will be fast because the agent crashes quickly!\")\n",
    "\n",
    "# 1. Create the environment\n",
    "env = gym.make('highway-v0', render_mode='human')\n",
    "\n",
    "# Start the main loop to run N_EPISODES\n",
    "for i in range(N_EPISODES):\n",
    "    \n",
    "    # --- Per-Episode Counters ---\n",
    "    current_episode_reward = 0\n",
    "    current_episode_steps = 0\n",
    "    current_episode_speed_sum = 0\n",
    "    current_episode_lane_changes = 0\n",
    "    current_episode_speed_limit_violations = 0\n",
    "    \n",
    "    # NEW: Track acceleration for jerk calculation\n",
    "    previous_acceleration = 0.0\n",
    "    jerk_values = []\n",
    "    \n",
    "    # NEW: Track Time-to-Collision (TTC)\n",
    "    ttc_values = []\n",
    "    \n",
    "    # NEW: Track episode start time\n",
    "    episode_start_time = time.time()\n",
    "    \n",
    "    # NEW: Track previous speed for acceleration calculation\n",
    "    previous_speed = 0.0\n",
    "    \n",
    "    # 2. Reset the environment for a new episode\n",
    "    obs, info = env.reset()\n",
    "    done = truncated = False\n",
    "    \n",
    "    # 3. Inner loop: run one full episode\n",
    "    while not (done or truncated):\n",
    "        \n",
    "        # --- The \"Brain\": A Random Agent ---\n",
    "        action = env.action_space.sample() \n",
    "        \n",
    "        # 4. Take the action\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # --- Collect Metrics ---\n",
    "        current_episode_reward += reward\n",
    "        current_episode_steps += 1\n",
    "        \n",
    "        # Get Ego Vehicle speed (it's the first vehicle in the observation)\n",
    "        # obs[0] is the ego car, obs[0][3] is its 'vx' (longitudinal velocity)\n",
    "        ego_speed = obs[0][3] \n",
    "        current_episode_speed_sum += ego_speed\n",
    "        \n",
    "        # NEW: Calculate acceleration and jerk\n",
    "        acceleration = ego_speed - previous_speed\n",
    "        jerk = acceleration - previous_acceleration\n",
    "        jerk_values.append(abs(jerk))\n",
    "        previous_acceleration = acceleration\n",
    "        previous_speed = ego_speed\n",
    "        \n",
    "        # NEW: Calculate Time-to-Collision (TTC) with nearest vehicle\n",
    "        # Check all other vehicles in observation\n",
    "        min_ttc_this_step = float('inf')\n",
    "        ego_x = obs[0][1]  # Ego longitudinal position\n",
    "        ego_y = obs[0][2]  # Ego lateral position\n",
    "        \n",
    "        for vehicle_idx in range(1, len(obs)):  # Skip ego (index 0)\n",
    "            if obs[vehicle_idx][0] == 0:  # Check if vehicle exists (presence flag)\n",
    "                continue\n",
    "                \n",
    "            other_x = obs[vehicle_idx][1]\n",
    "            other_y = obs[vehicle_idx][2]\n",
    "            other_vx = obs[vehicle_idx][3]\n",
    "            \n",
    "            # Only consider vehicles ahead in the same or adjacent lanes\n",
    "            relative_x = other_x - ego_x\n",
    "            relative_y = abs(other_y - ego_y)\n",
    "            \n",
    "            if relative_x > 0 and relative_y < 0.2:  # Vehicle ahead, similar lane\n",
    "                relative_velocity = ego_speed - other_vx\n",
    "                \n",
    "                if relative_velocity > 0.01:  # We're approaching them\n",
    "                    ttc = relative_x / relative_velocity\n",
    "                    if ttc > 0:\n",
    "                        min_ttc_this_step = min(min_ttc_this_step, ttc)\n",
    "        \n",
    "        if min_ttc_this_step != float('inf'):\n",
    "            ttc_values.append(min_ttc_this_step)\n",
    "        \n",
    "        # Check for lane change\n",
    "        if action == 0 or action == 2: # 0=LANE_LEFT, 2=LANE_RIGHT\n",
    "            current_episode_lane_changes += 1\n",
    "            \n",
    "        # Check for speed limit compliance\n",
    "        if ego_speed > SPEED_LIMIT_MS:\n",
    "            current_episode_speed_limit_violations += 1\n",
    "\n",
    "    # --- Episode Finished: Calculate Stats ---\n",
    "    episode_duration = time.time() - episode_start_time\n",
    "    \n",
    "    # Check termination reason\n",
    "    was_collision = info.get('crashed', False)\n",
    "    was_success = not was_collision and not truncated  # Reached goal safely\n",
    "    \n",
    "    # Calculate averages for this episode\n",
    "    avg_speed = current_episode_speed_sum / (current_episode_steps + 1e-6)\n",
    "    speed_compliance_frac = 1.0 - (current_episode_speed_limit_violations / (current_episode_steps + 1e-6))\n",
    "    \n",
    "    # NEW: Calculate jerk statistics\n",
    "    avg_jerk = np.mean(jerk_values) if jerk_values else 0.0\n",
    "    max_jerk = np.max(jerk_values) if jerk_values else 0.0\n",
    "    \n",
    "    # NEW: Calculate TTC statistics\n",
    "    avg_ttc = np.mean(ttc_values) if ttc_values else float('inf')\n",
    "    min_ttc = np.min(ttc_values) if ttc_values else float('inf')\n",
    "    ttc_violations = sum(1 for ttc in ttc_values if ttc < SAFE_TTC_THRESHOLD)\n",
    "    ttc_violation_rate = ttc_violations / (current_episode_steps + 1e-6)\n",
    "    \n",
    "    # Store all metrics in a dictionary\n",
    "    stats = {\n",
    "        \"episode\": i + 1,\n",
    "        \"collision\": was_collision,\n",
    "        \"success\": was_success,\n",
    "        \"total_reward\": current_episode_reward,\n",
    "        \"episode_duration_s\": episode_duration,\n",
    "        \"steps\": current_episode_steps,\n",
    "        \"avg_speed_ms\": avg_speed,\n",
    "        \"lane_changes\": current_episode_lane_changes,\n",
    "        \"speed_compliance\": speed_compliance_frac,\n",
    "        \"avg_jerk\": avg_jerk,\n",
    "        \"max_jerk\": max_jerk,\n",
    "        \"avg_ttc\": avg_ttc if avg_ttc != float('inf') else -1,  # -1 means no TTC data\n",
    "        \"min_ttc\": min_ttc if min_ttc != float('inf') else -1,\n",
    "        \"ttc_violation_rate\": ttc_violation_rate\n",
    "    }\n",
    "    all_episode_stats.append(stats)\n",
    "    \n",
    "    # Print progress\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"  ... Episode {i+1} / {N_EPISODES} complete.\")\n",
    "\n",
    "# --- All Episodes Done: Final Report ---\n",
    "env.close()\n",
    "print(\"\\n--- Evaluation Complete. ---\")\n",
    "\n",
    "# Convert the list of dictionaries into a pandas DataFrame\n",
    "df = pd.DataFrame(all_episode_stats)\n",
    "\n",
    "# Save the raw DataFrame to the 'instant_runs' directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"run_{timestamp}.csv\"\n",
    "output_path = os.path.join(RAW_RUNS_DIR, filename)\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Raw results for all {N_EPISODES} episodes saved to: {output_path}\")\n",
    "\n",
    "# --- Calculate and Print Final Metrics Summary ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"--- ENHANCED METRICS SUMMARY (Random Agent Baseline) ---\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Basic metrics\n",
    "final_metrics = {\n",
    "    \"Collision Rate\": df['collision'].mean(),\n",
    "    \"Success Rate\": df['success'].mean(),\n",
    "    \"Average Speed (m/s)\": df['avg_speed_ms'].mean(),\n",
    "    \"Avg Episode Duration (s)\": df['episode_duration_s'].mean(),\n",
    "    \"Avg Steps per Episode\": df['steps'].mean(),\n",
    "    \"Avg Lane Changes / Episode\": df['lane_changes'].mean(),\n",
    "    \"Average Reward / Episode\": df['total_reward'].mean(),\n",
    "    \"Speed Limit Compliance\": df['speed_compliance'].mean(),\n",
    "    \"Avg Jerk (Comfort)\": df['avg_jerk'].mean(),\n",
    "    \"Max Jerk (Worst Case)\": df['max_jerk'].mean(),\n",
    "    \"Avg Time-to-Collision (s)\": df[df['avg_ttc'] > 0]['avg_ttc'].mean() if len(df[df['avg_ttc'] > 0]) > 0 else -1,\n",
    "    \"Min Time-to-Collision (s)\": df[df['min_ttc'] > 0]['min_ttc'].mean() if len(df[df['min_ttc'] > 0]) > 0 else -1,\n",
    "    \"TTC Violation Rate\": df['ttc_violation_rate'].mean()\n",
    "}\n",
    "\n",
    "# Save the summary metrics\n",
    "df_summary = pd.DataFrame(list(final_metrics.items()), columns=['Metric', 'Value'])\n",
    "summary_filename = f\"summary_{timestamp}.csv\"\n",
    "summary_output_path = os.path.join(SUMMARY_DIR, summary_filename)\n",
    "df_summary.to_csv(summary_output_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Summary metrics file saved to: {summary_output_path}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# Print as a clean table\n",
    "for metric, value in final_metrics.items():\n",
    "    if value == -1:\n",
    "        print(f\"{metric:<35} | {'N/A':>12}\")\n",
    "    else:\n",
    "        print(f\"{metric:<35} | {value:>12.4f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüéØ Metrics saved! Continue to next cells for indicator analysis.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f568a3e9-b01d-4ac3-bbd8-ef58effcb745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54fbca82",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä PART 2: Performance Indicators Calculation\n",
    "\n",
    "Now we calculate the 5 key performance indicators defined in README.md Section 7-8:\n",
    "\n",
    "1. **Safety Index (SI)** - Combines collision rate, TTC, and violations\n",
    "2. **Efficiency Index (EI)** - Speed and success rate\n",
    "3. **Comfort Index (CI)** - Smoothness (inverse of jerk and lane changes)\n",
    "4. **Rule Compliance Index (RCI)** - Traffic rule adherence\n",
    "5. **Global Performance Score (GPS)** - Weighted aggregate of all indices\n",
    "\n",
    "### Indicator Weights (from research priorities):\n",
    "- GPS = **40% Safety** + **30% Efficiency** + **15% Comfort** + **15% Compliance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128346ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INDICATOR CALCULATION MODULE\n",
    "# ============================================================================\n",
    "# This section calculates performance indicators from the metrics collected above.\n",
    "# These indicators provide a comprehensive evaluation framework for comparing\n",
    "# different RL models (Random Baseline, PPO, DQN, SAC).\n",
    "\n",
    "# --- Configuration: Indicator Weights ---\n",
    "INDICATOR_WEIGHTS = {\n",
    "    'safety_index': {\n",
    "        'w_collision': 0.4,      # Weight for collision avoidance\n",
    "        'w_ttc': 0.4,            # Weight for Time-to-Collision\n",
    "        'w_ttc_violations': 0.2  # Weight for TTC violation rate\n",
    "    },\n",
    "    'efficiency_index': {\n",
    "        'w_speed': 0.5,          # Weight for average speed\n",
    "        'w_success': 0.5         # Weight for success rate\n",
    "    },\n",
    "    'comfort_index': {\n",
    "        'w_jerk': 0.6,           # Weight for jerk (smoothness)\n",
    "        'w_lane_changes': 0.4    # Weight for lane change frequency\n",
    "    },\n",
    "    'rule_compliance_index': {\n",
    "        'w_speed_compliance': 1.0  # Weight for speed limit adherence\n",
    "    },\n",
    "    'global_performance': {\n",
    "        'a_safety': 0.40,        # Weight for Safety Index\n",
    "        'b_efficiency': 0.30,    # Weight for Efficiency Index\n",
    "        'c_comfort': 0.15,       # Weight for Comfort Index\n",
    "        'd_compliance': 0.15     # Weight for Rule Compliance Index\n",
    "    }\n",
    "}\n",
    "\n",
    "# Normalization constants\n",
    "MAX_EXPECTED_JERK = 10.0\n",
    "MAX_EXPECTED_LANE_CHANGES = 20.0\n",
    "\n",
    "\n",
    "# --- Helper Function: Extract Metric Value ---\n",
    "def get_metric_value(metrics_dict, metric_name, default=0.0):\n",
    "    \"\"\"Safely extract a metric value from the dictionary.\"\"\"\n",
    "    return metrics_dict.get(metric_name, default)\n",
    "\n",
    "\n",
    "# --- 1. Safety Index (SI) ---\n",
    "def calculate_safety_index(metrics):\n",
    "    \"\"\"\n",
    "    Safety Index = w1*(1 - CollisionRate) + w2*(TTC_norm) + w3*(1 - TTC_ViolationRate)\n",
    "    Range: [0, 1], Higher is safer\n",
    "    \"\"\"\n",
    "    w = INDICATOR_WEIGHTS['safety_index']\n",
    "    \n",
    "    collision_rate = get_metric_value(metrics, \"Collision Rate\")\n",
    "    avg_ttc = get_metric_value(metrics, \"Avg Time-to-Collision (s)\", -1)\n",
    "    ttc_violation_rate = get_metric_value(metrics, \"TTC Violation Rate\")\n",
    "    \n",
    "    # Normalize TTC to [0, 1]\n",
    "    if avg_ttc > 0:\n",
    "        ttc_normalized = min(avg_ttc / SAFE_TTC_THRESHOLD, 1.0)\n",
    "    else:\n",
    "        ttc_normalized = 0.0\n",
    "    \n",
    "    safety_index = (\n",
    "        w['w_collision'] * (1 - collision_rate) +\n",
    "        w['w_ttc'] * ttc_normalized +\n",
    "        w['w_ttc_violations'] * (1 - ttc_violation_rate)\n",
    "    )\n",
    "    \n",
    "    return safety_index\n",
    "\n",
    "\n",
    "# --- 2. Efficiency Index (EI) ---\n",
    "def calculate_efficiency_index(metrics):\n",
    "    \"\"\"\n",
    "    Efficiency Index = w1*(avg_speed / speed_limit) + w2*(SuccessRate)\n",
    "    Range: [0, 1], Higher is more efficient\n",
    "    \"\"\"\n",
    "    w = INDICATOR_WEIGHTS['efficiency_index']\n",
    "    \n",
    "    avg_speed = get_metric_value(metrics, \"Average Speed (m/s)\")\n",
    "    success_rate = get_metric_value(metrics, \"Success Rate\")\n",
    "    \n",
    "    speed_ratio = min(avg_speed / SPEED_LIMIT_MS, 1.0)\n",
    "    \n",
    "    efficiency_index = (\n",
    "        w['w_speed'] * speed_ratio +\n",
    "        w['w_success'] * success_rate\n",
    "    )\n",
    "    \n",
    "    return efficiency_index\n",
    "\n",
    "\n",
    "# --- 3. Comfort Index (CI) ---\n",
    "def calculate_comfort_index(metrics):\n",
    "    \"\"\"\n",
    "    Comfort Index = 1 - (w1*Jerk_norm + w2*LaneChanges_norm)\n",
    "    Range: [0, 1], Higher is more comfortable (smoother)\n",
    "    \"\"\"\n",
    "    w = INDICATOR_WEIGHTS['comfort_index']\n",
    "    \n",
    "    avg_jerk = get_metric_value(metrics, \"Avg Jerk (Comfort)\")\n",
    "    lane_changes = get_metric_value(metrics, \"Avg Lane Changes / Episode\")\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    jerk_normalized = min(avg_jerk / MAX_EXPECTED_JERK, 1.0)\n",
    "    lane_changes_normalized = min(lane_changes / MAX_EXPECTED_LANE_CHANGES, 1.0)\n",
    "    \n",
    "    # Calculate discomfort, then invert\n",
    "    discomfort = w['w_jerk'] * jerk_normalized + w['w_lane_changes'] * lane_changes_normalized\n",
    "    comfort_index = max(1 - discomfort, 0.0)  # Ensure non-negative\n",
    "    \n",
    "    return comfort_index\n",
    "\n",
    "\n",
    "# --- 4. Rule Compliance Index (RCI) ---\n",
    "def calculate_rule_compliance_index(metrics):\n",
    "    \"\"\"\n",
    "    Rule Compliance Index = SpeedLimitCompliance\n",
    "    Range: [0, 1], Higher is better compliance\n",
    "    \"\"\"\n",
    "    speed_compliance = get_metric_value(metrics, \"Speed Limit Compliance\")\n",
    "    return speed_compliance\n",
    "\n",
    "\n",
    "# --- 5. Global Performance Score (GPS) ---\n",
    "def calculate_global_performance_score(si, ei, ci, rci):\n",
    "    \"\"\"\n",
    "    Global Performance Score = a*SI + b*EI + c*CI + d*RCI\n",
    "    Range: [0, 1], Higher is better overall performance\n",
    "    \"\"\"\n",
    "    w = INDICATOR_WEIGHTS['global_performance']\n",
    "    \n",
    "    gps = (\n",
    "        w['a_safety'] * si +\n",
    "        w['b_efficiency'] * ei +\n",
    "        w['c_comfort'] * ci +\n",
    "        w['d_compliance'] * rci\n",
    "    )\n",
    "    \n",
    "    return gps\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CALCULATE ALL INDICATORS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üî¨ CALCULATING PERFORMANCE INDICATORS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate each indicator\n",
    "SI = calculate_safety_index(final_metrics)\n",
    "EI = calculate_efficiency_index(final_metrics)\n",
    "CI = calculate_comfort_index(final_metrics)\n",
    "RCI = calculate_rule_compliance_index(final_metrics)\n",
    "GPS = calculate_global_performance_score(SI, EI, CI, RCI)\n",
    "\n",
    "# Store indicators\n",
    "indicators = {\n",
    "    \"Safety Index (SI)\": SI,\n",
    "    \"Efficiency Index (EI)\": EI,\n",
    "    \"Comfort Index (CI)\": CI,\n",
    "    \"Rule Compliance Index (RCI)\": RCI,\n",
    "    \"Global Performance Score (GPS)\": GPS\n",
    "}\n",
    "\n",
    "# Print indicators\n",
    "print(\"\\nüìä PERFORMANCE INDICATORS:\")\n",
    "print(\"-\" * 70)\n",
    "for indicator_name, value in indicators.items():\n",
    "    # Add emoji based on performance\n",
    "    if value >= 0.75:\n",
    "        emoji = \"üü¢\"\n",
    "    elif value >= 0.50:\n",
    "        emoji = \"üü°\"\n",
    "    elif value >= 0.25:\n",
    "        emoji = \"üü†\"\n",
    "    else:\n",
    "        emoji = \"üî¥\"\n",
    "    \n",
    "    print(f\"{emoji} {indicator_name:<35} | {value:>8.4f}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"\\nüéØ OVERALL PERFORMANCE: {GPS:.4f} / 1.0000\")\n",
    "\n",
    "# Interpret GPS\n",
    "if GPS >= 0.80:\n",
    "    performance_label = \"üåü EXCELLENT - Ready for deployment\"\n",
    "elif GPS >= 0.60:\n",
    "    performance_label = \"‚úÖ GOOD - Acceptable performance\"\n",
    "elif GPS >= 0.40:\n",
    "    performance_label = \"‚ö†Ô∏è  FAIR - Needs improvement\"\n",
    "else:\n",
    "    performance_label = \"‚ùå POOR - Requires significant work\"\n",
    "\n",
    "print(f\"   {performance_label}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE INDICATORS TO FILE\n",
    "# ============================================================================\n",
    "\n",
    "INDICATORS_DIR = os.path.join(BASE_OUTPUT_DIR, \"indicators\")\n",
    "os.makedirs(INDICATORS_DIR, exist_ok=True)\n",
    "\n",
    "indicators_df = pd.DataFrame(list(indicators.items()), columns=['Indicator', 'Value'])\n",
    "indicators_filename = f\"indicators_{timestamp}.csv\"\n",
    "indicators_output_path = os.path.join(INDICATORS_DIR, indicators_filename)\n",
    "indicators_df.to_csv(indicators_output_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Indicators saved to: {indicators_output_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# BREAKDOWN ANALYSIS (For Understanding)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üîç INDICATOR BREAKDOWN (What drives each score?)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  SAFETY INDEX (SI) = {:.4f}\".format(SI))\n",
    "print(\"    Components:\")\n",
    "print(f\"    - Collision Avoidance : {1 - get_metric_value(final_metrics, 'Collision Rate'):.4f} (weight: 40%)\")\n",
    "avg_ttc = get_metric_value(final_metrics, \"Avg Time-to-Collision (s)\", -1)\n",
    "ttc_norm = min(avg_ttc / SAFE_TTC_THRESHOLD, 1.0) if avg_ttc > 0 else 0.0\n",
    "print(f\"    - Time-to-Collision   : {ttc_norm:.4f} (weight: 40%)\")\n",
    "print(f\"    - TTC Safety Margin   : {1 - get_metric_value(final_metrics, 'TTC Violation Rate'):.4f} (weight: 20%)\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  EFFICIENCY INDEX (EI) = {:.4f}\".format(EI))\n",
    "print(\"    Components:\")\n",
    "print(f\"    - Speed Ratio         : {get_metric_value(final_metrics, 'Average Speed (m/s)') / SPEED_LIMIT_MS:.4f} (weight: 50%)\")\n",
    "print(f\"    - Success Rate        : {get_metric_value(final_metrics, 'Success Rate'):.4f} (weight: 50%)\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£  COMFORT INDEX (CI) = {:.4f}\".format(CI))\n",
    "print(\"    Components:\")\n",
    "jerk_norm = min(get_metric_value(final_metrics, \"Avg Jerk (Comfort)\") / MAX_EXPECTED_JERK, 1.0)\n",
    "lc_norm = min(get_metric_value(final_metrics, \"Avg Lane Changes / Episode\") / MAX_EXPECTED_LANE_CHANGES, 1.0)\n",
    "print(f\"    - Smoothness (1-Jerk) : {1 - jerk_norm:.4f} (weight: 60%)\")\n",
    "print(f\"    - Lane Stability      : {1 - lc_norm:.4f} (weight: 40%)\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£  RULE COMPLIANCE INDEX (RCI) = {:.4f}\".format(RCI))\n",
    "print(\"    Components:\")\n",
    "print(f\"    - Speed Compliance    : {get_metric_value(final_metrics, 'Speed Limit Compliance'):.4f} (weight: 100%)\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£  GLOBAL PERFORMANCE SCORE (GPS) = {:.4f}\".format(GPS))\n",
    "print(\"    Weighted Combination:\")\n",
    "print(f\"    - Safety      ({INDICATOR_WEIGHTS['global_performance']['a_safety']:.0%})  : {SI:.4f} ‚Üí {SI * INDICATOR_WEIGHTS['global_performance']['a_safety']:.4f}\")\n",
    "print(f\"    - Efficiency  ({INDICATOR_WEIGHTS['global_performance']['b_efficiency']:.0%})  : {EI:.4f} ‚Üí {EI * INDICATOR_WEIGHTS['global_performance']['b_efficiency']:.4f}\")\n",
    "print(f\"    - Comfort     ({INDICATOR_WEIGHTS['global_performance']['c_comfort']:.0%})  : {CI:.4f} ‚Üí {CI * INDICATOR_WEIGHTS['global_performance']['c_comfort']:.4f}\")\n",
    "print(f\"    - Compliance  ({INDICATOR_WEIGHTS['global_performance']['d_compliance']:.0%})  : {RCI:.4f} ‚Üí {RCI * INDICATOR_WEIGHTS['global_performance']['d_compliance']:.4f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚ú® Analysis Complete! All metrics and indicators saved.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27441fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà PART 3: Visualization (Optional)\n",
    "\n",
    "Uncomment and run the cell below to create a visual comparison chart of all indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96dc891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION: Bar Chart of Indicators\n",
    "# ============================================================================\n",
    "# Uncomment the code below to create a visual chart\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for plotting\n",
    "indicator_names = ['Safety\\nIndex', 'Efficiency\\nIndex', 'Comfort\\nIndex', \n",
    "                   'Rule\\nCompliance', 'Global\\nPerformance']\n",
    "indicator_values = [SI, EI, CI, RCI, GPS]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "\n",
    "# Create bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(indicator_names, indicator_values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, indicator_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{value:.3f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Styling\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_ylabel('Score (0-1)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Performance Indicators - Random Baseline Agent', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, linewidth=1)\n",
    "ax.axhline(y=0.75, color='green', linestyle='--', alpha=0.3, linewidth=1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add reference lines legend\n",
    "ax.text(0.02, 0.51, 'Acceptable (0.5)', transform=ax.transAxes, \n",
    "        fontsize=9, color='gray', style='italic')\n",
    "ax.text(0.02, 0.76, 'Good (0.75)', transform=ax.transAxes, \n",
    "        fontsize=9, color='green', style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1673ad04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèÜ PART 4: Multi-Model Comparison (Optional)\n",
    "\n",
    "**After running multiple models**, use the cell below to compare them all side-by-side.\n",
    "\n",
    "This replaces the need for the separate `analyze_results.py` script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3914a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MULTI-MODEL COMPARISON\n",
    "# ============================================================================\n",
    "# Run this cell AFTER you've completed all model runs (Random, PPO, DQN, SAC)\n",
    "# to generate a side-by-side comparison table\n",
    "\n",
    "import glob\n",
    "\n",
    "# --- Configuration ---\n",
    "MODELS_TO_COMPARE = [\n",
    "    (\"Random Baseline\", \"random_baseline_agent/summary/summary_*.csv\"),\n",
    "    (\"PPO Agent\", \"ppo_agent/summary/summary_*.csv\"),\n",
    "    (\"DQN Agent\", \"dqn_agent/summary/summary_*.csv\"),\n",
    "    (\"SAC Agent\", \"sac_agent/summary/summary_*.csv\"),\n",
    "]\n",
    "\n",
    "# Enable comparison mode\n",
    "RUN_COMPARISON = False  # Set to True when you want to compare models\n",
    "\n",
    "if RUN_COMPARISON:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üèÜ MULTI-MODEL COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for model_name, csv_pattern in MODELS_TO_COMPARE:\n",
    "        # Find the most recent CSV for this model\n",
    "        csv_files = glob.glob(csv_pattern)\n",
    "        \n",
    "        if not csv_files:\n",
    "            print(f\"\\n‚ö†Ô∏è  No data found for {model_name} (pattern: {csv_pattern})\")\n",
    "            continue\n",
    "        \n",
    "        # Get the most recent file\n",
    "        latest_csv = max(csv_files, key=os.path.getctime)\n",
    "        \n",
    "        print(f\"\\nüìä Loading {model_name} from: {os.path.basename(latest_csv)}\")\n",
    "        \n",
    "        # Load metrics\n",
    "        model_metrics_df = pd.read_csv(latest_csv)\n",
    "        model_metrics = dict(zip(model_metrics_df['Metric'], model_metrics_df['Value']))\n",
    "        \n",
    "        # Calculate indicators for this model\n",
    "        model_SI = calculate_safety_index(model_metrics)\n",
    "        model_EI = calculate_efficiency_index(model_metrics)\n",
    "        model_CI = calculate_comfort_index(model_metrics)\n",
    "        model_RCI = calculate_rule_compliance_index(model_metrics)\n",
    "        model_GPS = calculate_global_performance_score(model_SI, model_EI, model_CI, model_RCI)\n",
    "        \n",
    "        # Store results\n",
    "        comparison_results.append({\n",
    "            'Model': model_name,\n",
    "            'Safety_Index': model_SI,\n",
    "            'Efficiency_Index': model_EI,\n",
    "            'Comfort_Index': model_CI,\n",
    "            'Rule_Compliance_Index': model_RCI,\n",
    "            'Global_Performance_Score': model_GPS,\n",
    "            'Collision_Rate': get_metric_value(model_metrics, \"Collision Rate\"),\n",
    "            'Success_Rate': get_metric_value(model_metrics, \"Success Rate\"),\n",
    "            'Avg_Speed': get_metric_value(model_metrics, \"Average Speed (m/s)\"),\n",
    "            'Avg_Reward': get_metric_value(model_metrics, \"Average Reward / Episode\")\n",
    "        })\n",
    "    \n",
    "    if comparison_results:\n",
    "        # Create comparison DataFrame\n",
    "        comparison_df = pd.DataFrame(comparison_results)\n",
    "        \n",
    "        # Display comparison table\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä PERFORMANCE COMPARISON TABLE\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\n--- Key Indicators ---\")\n",
    "        print(comparison_df[['Model', 'Global_Performance_Score', 'Safety_Index', \n",
    "                             'Efficiency_Index', 'Comfort_Index', 'Rule_Compliance_Index']].to_string(index=False))\n",
    "        \n",
    "        print(\"\\n--- Raw Metrics ---\")\n",
    "        print(comparison_df[['Model', 'Collision_Rate', 'Success_Rate', \n",
    "                             'Avg_Speed', 'Avg_Reward']].to_string(index=False))\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Highlight best performers\n",
    "        print(\"\\nü•á BEST PERFORMERS:\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        best_performers = {\n",
    "            'Global Performance': comparison_df.loc[comparison_df['Global_Performance_Score'].idxmax()],\n",
    "            'Safety': comparison_df.loc[comparison_df['Safety_Index'].idxmax()],\n",
    "            'Efficiency': comparison_df.loc[comparison_df['Efficiency_Index'].idxmax()],\n",
    "            'Comfort': comparison_df.loc[comparison_df['Comfort_Index'].idxmax()],\n",
    "            'Compliance': comparison_df.loc[comparison_df['Rule_Compliance_Index'].idxmax()],\n",
    "        }\n",
    "        \n",
    "        for category, best_model in best_performers.items():\n",
    "            if category == 'Global Performance':\n",
    "                score = best_model['Global_Performance_Score']\n",
    "                metric = 'GPS'\n",
    "            elif category == 'Safety':\n",
    "                score = best_model['Safety_Index']\n",
    "                metric = 'SI'\n",
    "            elif category == 'Efficiency':\n",
    "                score = best_model['Efficiency_Index']\n",
    "                metric = 'EI'\n",
    "            elif category == 'Comfort':\n",
    "                score = best_model['Comfort_Index']\n",
    "                metric = 'CI'\n",
    "            else:\n",
    "                score = best_model['Rule_Compliance_Index']\n",
    "                metric = 'RCI'\n",
    "            \n",
    "            print(f\"   {category:<20}: {best_model['Model']:<20} ({metric} = {score:.4f})\")\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Save comparison results\n",
    "        comparison_output_dir = \"model_comparison\"\n",
    "        os.makedirs(comparison_output_dir, exist_ok=True)\n",
    "        comparison_output_path = os.path.join(comparison_output_dir, \n",
    "                                              f\"comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "        comparison_df.to_csv(comparison_output_path, index=False)\n",
    "        print(f\"\\n‚úÖ Comparison results saved to: {comparison_output_path}\")\n",
    "        \n",
    "        # Optional: Create comparison visualization\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            # Prepare data for grouped bar chart\n",
    "            models = comparison_df['Model'].tolist()\n",
    "            x = np.arange(len(models))\n",
    "            width = 0.15\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(14, 7))\n",
    "            \n",
    "            # Plot bars for each indicator\n",
    "            bars1 = ax.bar(x - 2*width, comparison_df['Safety_Index'], width, label='Safety', color='#FF6B6B', alpha=0.8)\n",
    "            bars2 = ax.bar(x - width, comparison_df['Efficiency_Index'], width, label='Efficiency', color='#4ECDC4', alpha=0.8)\n",
    "            bars3 = ax.bar(x, comparison_df['Comfort_Index'], width, label='Comfort', color='#45B7D1', alpha=0.8)\n",
    "            bars4 = ax.bar(x + width, comparison_df['Rule_Compliance_Index'], width, label='Compliance', color='#FFA07A', alpha=0.8)\n",
    "            bars5 = ax.bar(x + 2*width, comparison_df['Global_Performance_Score'], width, label='GPS', color='#98D8C8', alpha=0.8, edgecolor='black', linewidth=2)\n",
    "            \n",
    "            # Styling\n",
    "            ax.set_ylabel('Score (0-1)', fontsize=12, fontweight='bold')\n",
    "            ax.set_title('Multi-Model Performance Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(models, fontsize=11)\n",
    "            ax.legend(loc='upper left', fontsize=10)\n",
    "            ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.3, linewidth=1)\n",
    "            ax.axhline(y=0.75, color='green', linestyle='--', alpha=0.3, linewidth=1)\n",
    "            ax.set_ylim(0, 1.0)\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"\\nüìä Comparison visualization complete!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  Could not create visualization: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n‚ùå No model data found. Make sure you've run simulations for at least one model.\")\n",
    "        print(\"   Expected directory structure:\")\n",
    "        for model_name, csv_pattern in MODELS_TO_COMPARE:\n",
    "            print(f\"   - {csv_pattern}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nüí° TIP: To compare multiple models:\")\n",
    "    print(\"   1. Run this notebook for each model (Random, PPO, DQN, SAC)\")\n",
    "    print(\"   2. Set RUN_COMPARISON = True in this cell\")\n",
    "    print(\"   3. Re-run this cell to see side-by-side comparison!\")\n",
    "    print(\"\\n   This replaces the need for analyze_results.py üéØ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
