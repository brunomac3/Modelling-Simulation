{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78254c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.ipynb\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "import os\n",
    "\n",
    "TOTAL_TIMESTEPS = 500_000          # ↑ Increased\n",
    "AGENT_NAME = \"ppo_agent\"\n",
    "os.makedirs(AGENT_NAME, exist_ok=True)\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"highway-v0\")\n",
    "    env.unwrapped.config.update({\n",
    "\n",
    "        \"observation\": {\"type\": \"Kinematics\"},\n",
    "        \"vehicles_count\": 35,\n",
    "        \"duration\": 200,\n",
    "        \"simulation_frequency\": 15,\n",
    "        \"policy_frequency\": 5,\n",
    "        \"reward_speed_range\": [20, 32],\n",
    "        \"collision_penalty\": -8,\n",
    "        \"right_lane_reward\": 0.1,\n",
    "        \"lane_change_reward\": -0.03,\n",
    "        \"high_speed_reward\": 0.6,\n",
    "        \"offscreen_rendering\": True    # ⚡ HUGE BOOST\n",
    "    })\n",
    "    return env\n",
    "\n",
    "# Vectorized + normalization\n",
    "env = DummyVecEnv([make_env])\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
    "\n",
    "# === Improved PPO hyperparameters ===\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    n_steps=2048,                 # ↑ much better with PPO\n",
    "    batch_size=256,\n",
    "    gae_lambda=0.95,\n",
    "    gamma=0.99,\n",
    "    n_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=[256, 256, 128]  # ↑ bigger network\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Train\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "\n",
    "# Save\n",
    "model.save(os.path.join(AGENT_NAME, \"model\"))\n",
    "env.save(os.path.join(AGENT_NAME, \"vec_normalize.pkl\"))\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa889c8d-e502-488f-9265-34d1412c5953",
   "metadata": {},
   "source": [
    "1. More vehicles → more realistic & difficult\n",
    "\n",
    "The agent must learn:\n",
    "\n",
    "safe following\n",
    "\n",
    "when to change lanes\n",
    "\n",
    "when to brake\n",
    "\n",
    "how to avoid blocks\n",
    "\n",
    "Sparse traffic doesn’t teach real overtaking.\n",
    "\n",
    "2. Longer episodes prevent early resets\n",
    "\n",
    "Short episodes limit learning — the agent keeps resetting before reaching complex situations.\n",
    "\n",
    "3. Reward shaping teaches good driving behavior\n",
    "\n",
    "Highway-Env agents often collapse to:\n",
    "\n",
    "staying in right lane\n",
    "\n",
    "going slow\n",
    "\n",
    "avoiding everything\n",
    "\n",
    "The improved reward balancing fixes this.\n",
    "\n",
    "4. PPO hyperparameters optimized\n",
    "\n",
    "Using the standard PPO recipe:\n",
    "\n",
    "n_steps=2048\n",
    "\n",
    "gamma=.99\n",
    "\n",
    "gae_lambda=.95\n",
    "\n",
    "clip_range=.2\n",
    "\n",
    "These are the same used for Ant, Humanoid, and other continuous control tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
